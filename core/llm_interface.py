import json
import time
from llama_cpp import Llama


class DJAILL:
    """Interface avec le LLM qui joue le r√¥le du DJ"""

    def __init__(self, model_path, config=None):
        """
        Initialise l'interface avec le LLM

        Args:
            model_path (str): Chemin vers le mod√®le GMA-4B
            profile_name (str): Nom du profil DJ √† utiliser
            config (dict): Configuration globale
        """
        self.model_path = model_path
        self.session_state = config

    def _init_model(self):
        """Initialise ou r√©initialise le mod√®le LLM"""
        print(f"‚ö° Initialisation du mod√®le LLM depuis {self.model_path}...")

        # Si un mod√®le existe d√©j√†, le d√©truire explicitement
        if hasattr(self, "model"):
            try:
                del self.model
                import gc

                gc.collect()  # Force la lib√©ration de m√©moire
                print("üßπ Ancien mod√®le d√©truit")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors de la destruction du mod√®le: {e}")

        # Initialiser un nouveau mod√®le LLM
        self.model = Llama(
            model_path=self.model_path,
            n_ctx=4096,  # Contexte suffisamment grand
            n_gpu_layers=-1,  # Utiliser tous les layers GPU disponibles
            n_threads=4,  # Ajuster selon CPU
            verbose=False,
        )
        print("‚úÖ Nouveau mod√®le LLM initialis√©")

    def get_next_decision(self):
        """Obtient la prochaine d√©cision du DJ IA"""

        # Mise √† jour du temps de session
        current_time = time.time()
        if self.session_state["last_action_time"] > 0:
            elapsed = current_time - self.session_state["last_action_time"]
            self.session_state["session_duration"] += elapsed
        self.session_state["last_action_time"] = current_time

        # G√©n√©rer la r√©ponse du LLM
        user_prompt = self._build_prompt()
        print("\nüß† G√©n√©ration AI-DJ prompt...")
        response = self.model.create_chat_completion(
            [
                {"role": "system", "content": self.get_system_prompt()},
                {"role": "user", "content": user_prompt},
            ]
        )
        print("‚úÖ G√©n√©ration termin√©e !")
        try:
            # Extraire et parser la r√©ponse JSON
            response_text = response["choices"][0]["message"]["content"]
            # Trouver le JSON dans la r√©ponse (au cas o√π le mod√®le ajoute du texte autour)
            import re

            json_match = re.search(r"({.*})", response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                decision = json.loads(json_str)
            else:
                # Fallback si pas de JSON trouv√©
                decision = {
                    "action_type": "sample",  # Action par d√©faut
                    "parameters": {"type": "techno_kick", "intensity": 5},
                    "reasoning": "Fallback: Pas de r√©ponse JSON valide",
                }
        except (json.JSONDecodeError, KeyError) as e:
            print(f"Erreur de parsing de la r√©ponse: {e}")
            print(f"R√©ponse brute: {response_text}")
            # D√©cision par d√©faut en cas d'erreur
            decision = {
                "action_type": "sample",
                "parameters": {"type": "techno_kick", "intensity": 5},
                "reasoning": f"Erreur: {str(e)}",
            }

        # Enregistrer la d√©cision dans l'historique
        self.session_state["history"].append(decision)

        return decision

    def _build_prompt(self):
        """Prompt user minimal"""
        special_instruction = self.session_state.get("special_instruction", "")

        return f"""Mots-cl√©s utilisateur: {special_instruction}"""

    def get_system_prompt(self) -> str:
        return """Tu es un g√©n√©rateur de samples musicaux. L'utilisateur te donne des mots-cl√©s, tu g√©n√®res un JSON simple.

    FORMAT OBLIGATOIRE :
    {
        "action_type": "generate_sample",
        "parameters": {
            "sample_details": {
                "musicgen_prompt": "[prompt optimis√© pour MusicGen bas√© sur les mots-cl√©s]",
                "key": "[tonalit√© appropri√©e ou garde celle fournie]"
            }
        },
        "reasoning": "Explication courte"
    }

    R√àGLES :
    - Cr√©e un prompt MusicGen coh√©rent √† partir des mots-cl√©s de l'user
    - Pour la tonalit√© : utilise celle fournie ou adapte si le style l'exige
    - R√©ponds UNIQUEMENT en JSON

    EXEMPLES :
    User: "ambient space" ‚Üí musicgen_prompt: "ambient atmospheric space soundscape, ethereal pads"
    User: "hard kick techno" ‚Üí musicgen_prompt: "hard techno kick, driving 4/4 beat, industrial"
    User: "jazzy piano" ‚Üí musicgen_prompt: "jazz piano, smooth chords, melodic improvisation"
    """
